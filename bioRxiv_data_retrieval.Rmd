---
title: "Examining the 'Early Access Effect' through bioRxiv preprints: data retrieval"
output: html_notebook
---

### This notebook provides code used to retrieve basic metadata records of bioRxiv preprints and their linked published papers from Crossref.


# Initial Configuration

```{r echo = T, results='hide'}
# Call libraries
library(tidyverse)
library(jsonlite)
library(lubridate)
library(curl)
library(httr)
  
# Credentials for APIs - these should be set in .Renviron file
email <- Sys.getenv("EMAIL")
altmetric_api_key <- Sys.getenv("ALTMETRIC_API_KEY")
```


```{r echo = T, results='hide'}
# Generate start and end dates of months for period of data extraction. Note that crossref API accepts dates in character format
startdates <- seq(ymd("2013-11-1"), ymd("2018-09-01"), by = "months")
enddates <- startdates %m+% months(1) - 1
months <- length(startdates)
```

# Retrieve Summary Data

### Total number of bioRxiv preprints deposited
```{r echo = T, results='hide'}
# Set start and end dates for analysis
startdate <- first(startdates)
enddate <- last(enddates)
  
# Query Crossref API for all preprints with a member id of 246 (corresponding to bioRxiv preprints) over time period
url <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,from-posted-date:", startdate, ",until-posted-date:", enddate, "&rows=0&mailto=", email, sep="")
data <- fromJSON(url)
  
#Extract counts
count_p_all <- data$message$`facets`$`publisher-name`$values$`Cold Spring Harbor Laboratory`
  
# Remove redundant variables
rm(url, data) 
```

### Total number of bioRxiv preprints deposited that have a linked published article in Crossref over time
```{r echo = T, results='hide'}
# Query Crossref API for all preprints with a member id of 246 that also have a preprint relationship to another published article (relation.type:is-preprint-of)
url <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,relation.type:is-preprint-of,from-posted-date:",startdate,",until-posted-date:",enddate,"&rows=0&mailto=", email, sep="")
data <- fromJSON(url)
  
# Extract counts
count_p_matching <- data$message$`facets`$`publisher-name`$values$`Cold Spring Harbor Laboratory`
  
# Remove redundant variables
rm(url, data, startdate, enddate) 
```

### Distribution of bioRxiv preprint deposits over time
```{r echo = T, results='hide'}
# Pre-allocate list for data
data = list()
  
# Progress bar
pb <- txtProgressBar(min = 0, max = months, initial= 0, style = 3)
  
# Iterate over months of analysis
for (i in 1:months){
  
  # Define URLs for API calls - separate calls for retrieving all preprints, and preprints that have a relationship to a published article
  url_all <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,from-posted-date:", startdates[i], ",until-posted-date:", enddates[i],"&rows=0&mailto=", email, sep="")
  url_related <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,relation.type:is-preprint-of,from-posted-date:", startdates[i], ",until-posted-date:", enddates[i], "&rows=0&mailto=", email, sep="")
  
  # Retrieve data
  response_all <- fromJSON(url_all)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  response_related <- fromJSON(url_related)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Build tibble of results
  data[[i]] <- tibble(
    "date" = format(as.Date(startdates[i]), "%Y-%m"),
    "preprints" = response_all$message$`total-results`,
    "matching_preprints" = response_related$message$`total-results`
  )
  
  setTxtProgressBar(pb, i)
  
}
  
# Bind data rows
preprint_distributions <- bind_rows(data)
  
# Remove redundant variables
rm(url_all, url_related, response_all, response_related, data, pb) 
```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(preprint_distributions, file="data/bioRxiv_preprints_monthly_distribution.csv", row.names=FALSE)
```

### Basic metadata for all bioRxiv preprints
```{r echo = T, results='hide'}
# API request function
request <- function(startdate, enddate, cursor){
  
  # URL for API call
  url <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,from-posted-date:", startdate, ",until-posted-date:", enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,relation,posted&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }  
}
  
# Crossref API can only serve 1000 rows per request. We loop over the dataset 1000 rows at a time and pass the cursor function to retrieve all results
loops <- ceiling(sum(preprint_distributions$preprints)/1000)
  
# Pre-allocate list to store data
metadata <- vector("list", length(loops))
  
# Set initial conditions
startdate <- first(startdates)
enddate <- last(enddates)
cursor <- "*"
  
# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)
  
for (i in 1:loops) {
  
  # Send request
  response <- request(startdate, enddate, cursor)
  
  # Retrieve data
  data <- content(response)
  data <- fromJSON(toJSON(data))
  
  # Update cursor - needs to be URL encoded
  cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
  
  # Parse basic metadata
  preprint_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  preprint_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  preprint_posted_date <- map(data$message$items$posted$`date-parts`, parseDate)
  
  # Build tibble of results
  metadata[[i]] <- tibble(
    "preprint_doi" = as.character(preprint_doi), 
    "preprint_title" = as.character(preprint_title), 
    "preprint_posted_date" = as.character(preprint_posted_date)
  )
  
  setTxtProgressBar(pb, i)
  
}
  
# Bind all data
all_preprints_metadata <- bind_rows(metadata)
  
# Remove redundant variables
rm(request, pb, i, loops, metadata, preprint_doi, preprint_title, preprint_posted_date, data, cursor, response, startdate, enddate, parseDate) 
```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(all_preprints_metadata, file="data/bioRxiv_all_preprints_metadata.csv", row.names=FALSE)
```

### Expanded metadata for bioRxiv preprints that have a link to a published paper in Crossref

```{r results='hide'}
# API request function
request <- function(startdate, enddate, cursor){
  
  # URL for API call
  url <- paste("http://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,relation.type:is-preprint-of,from-posted-date:",startdate,",until-posted-date:",enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,group-title,relation,posted&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }
}

# Crossref API can only serve 1000 rows per request. We loop over the dataset 1000 rows at a time and pass the cursor function to retrieve all results
loops <- ceiling(sum(preprint_distributions$matching_preprints)/1000)

# Pre-allocate list to store data
metadata <- vector("list", length(loops))

# Set initial conditions
startdate <- first(startdates)
enddate <- last(enddates)
cursor <- "*"

# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

for (i in 1:loops){
  
  # Send request
  response <- request(startdate, enddate, cursor)

  # Retrieve data
  data <- content(response)
  data <- fromJSON(toJSON(data))
  
  # Update cursor - needs to be URL encoded
  cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
  
  # Parse basic metadata
  preprint_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  preprint_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  data$message$items$`group-title`[sapply(data$message$items$`group-title`, is.null)] <- NA
  preprint_category <- unlist(data$message$items$`group-title`, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  preprint_posted_date <- map(data$message$items$posted$`date-parts`, parseDate)
  
  # Parse DOI of linked published paper
  parsePublishedDOI <- function(x){
    doi <- unlist(x[["id"]], recursive = TRUE, use.names = TRUE)
  }
  published_doi <- map(data$message$items$relation$`is-preprint-of`, parsePublishedDOI)
  
  # Build tibble of results
  metadata[[i]] <- tibble(
    "preprint_doi" = as.character(preprint_doi),
    "published_doi" = as.character(published_doi),
    "preprint_title" = as.character(preprint_title), 
    "preprint_posted_date" =  as.character(preprint_posted_date),
    "preprint_category" = as.character(preprint_category)
  )
  
  setTxtProgressBar(pb, i)
}

# Bind all data
matching_preprints_metadata <- bind_rows(metadata)

# Remove redundant variables
rm(request, pb, i, loops, metadata, preprint_doi, preprint_title, preprint_posted_date, preprint_category, preprint_published_doi, data, cursor, response, startdate, enddate) 
```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(matching_preprints_metadata, file="data/bioRxiv_matching_preprints_metadata.csv", row.names=FALSE)
```

### Metadata for published articles that are linked to bioRxiv preprints via Crossref. 
```{r results='hide'}
# API request function
request <- function(doi){
  
  # URL for API call
  url <- paste("http://api.crossref.org/works/", doi, "?mailto=", email, sep="")

  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }
}

# We will loop over each individual DOI
loops = length(matching_preprints_metadata$preprint_published_doi)

# Pre-allocate list to store data
metadata <- vector("list", length(loops))

# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

for (i in 1:loops){
  
  # Send request, ensure that DOI is character encoded
  response <- request(as.character(matching_preprints_metadata$published_doi[i]))
  
  # If no response, set all variables to empty
  if(!length(response)){
    
    preprint_doi <- matching_preprints_metadata$preprint_doi[i] 
    published_doi <- matching_preprints_metadata$published_doi[i] 
    published_publisher <- NA
    published_journal <- NA
    published_title <- NA
    published_type <- NA
    published_issue <- NA
    published_volume <- NA
    published_date <- NA
    published_issn <- NA
    
  } else{
    
    # Retrieve data
    data <- content(response)
    data <- fromJSON(toJSON(data))
  
    # Extract basic metadata
    preprint_doi <- matching_preprints_metadata$preprint_doi[i]
    published_doi <- if (length(data$message$DOI)) data$message$DOI else matching_preprints_metadata$published_doi[i]
    published_publisher <- if (length(data$message$publisher)) data$message$publisher else NA
    published_type <- if (length(data$message$type)) data$message$type else NA
    published_issue <- if (length(data$message$`journal-issue`$issue)) data$message$`journal-issue`$issue else NA
    published_volume <- if (length(data$message$volume)) data$message$volume else NA
    published_title <- if (length(data$message$title)) data$message$title[1] else NA
    published_journal <- if (length(data$message$`container-title`)) data$message$`container-title`[1] else NA
    published_issn <- if (length(data$message$ISSN)) data$message$ISSN[1] else NA
      
    # Extract created date - we use the crossref 'created-date' property as an indicator of when the article was first available through the journal page
    year <- data$message$created$`date-parts`[[1]]
    month <- data$message$created$`date-parts`[[2]]
    day <- data$message$created$`date-parts`[[3]]
    published_date <- paste(year, month, day, sep="-")
  }
  
  # Build tibble of results
  metadata[[i]] <- tibble(
    "preprint_doi" = as.character(preprint_doi),
    "published_doi" = as.character(published_doi), 
    "published_publisher" = as.character(published_publisher), 
    "published_journal" = as.character(published_journal), 
    "published_title" = as.character(published_title), 
    "published_type" = as.character(published_type), 
    "published_issue" = as.character(published_issue), 
    "published_volume" = as.character(published_volume), 
    "published_date" = as.character(published_date), 
    "published_issn" = as.character(published_issn)
  )

  setTxtProgressBar(pb, i)
  
}

# Bind all data
published_articles_metadata <- bind_rows(metadata)

# Remove redundant variables
rm(pb, i, year, month, day, df, published_date, published_title, published_type, published_issue, published_volume, published_publisher, published_doi, published_journal,published_issn, data, metadata, request, loops, response, parseDate, parsePublishedDOI)

```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(published_articles_metadata, file="data/bioRxiv_published_articles_metadata.csv", row.names=FALSE)
```

### Control dataset

### Generate a control dataset for comparative analysis. As a first step we retrieve all articles published in the same journal-months as our dataset of articles with preprints
```{r results='hide'}

# API request function
request <- function(startdate, enddate, issn, cursor){
  
  # URL for API call. 
  url <- paste("http://api.crossref.org/journals/", issn, "/works?filter=from-created-date:",startdate,",until-created-date:",enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,publisher,container-title,volume,issue,created,type&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response 
  if (response$status_code == 404 || !length(response)){ 
    return()
  } else  {
    return(response)
  }
}

parseData <- function(data){
  
  # Parse basic metadata
  control_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  data$message$items$title[sapply(data$message$items$title, is.null)] <- NA
  control_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  control_published_date <- map(data$message$items$created$`date-parts`, parseDate)
  
  # Build tibble of results
  d <- tibble(
    "published_doi" = as.character(published_articles$published_doi[i]),
    "control_doi" = as.character(control_doi), 
    "control_title" = as.character(control_title), 
    "control_published_date" = as.character(control_published_date),
    "control_results" = data$message$`total-results`
  )
  
  return(d)
}

published_articles <- read.csv("data/bioRxiv_published_articles_metadata.csv", header=TRUE)

# Loop over each article in the published dataset
loops <- length(published_articles$published_doi)

#Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

# Pre-allocate list to store data
 metadata <- vector("list", length(loops))

for(i in 1:loops) {
  
  # Set initial values
  startdate <- as.Date(published_articles$published_date[i])-15
  enddate <- as.Date(published_articles$published_date[i])+15
  issn <- as.character(published_articles$published_issn[i])
  cursor <- "*"
  
  # Send request
  response <- request(startdate, enddate, issn, cursor)
  
  setMetadataEmpty <- function(){
    metadata[[i]] <- tibble(
      "published_doi" = as.character(published_articles$published_doi[i]),
      "control_doi" = NA, 
      "control_title" = NA, 
      "control_published_date" = NA,
      "control_results" = NA
    )
    return(metadata[[i]])
  }
  
  # If no response, set values to empty
  if(!length(response)){
    setMetadataEmpty()
  } else {
    
    # Retrieve data
    data <- fromJSON(toJSON(content(response)))
    
    results <- data$message$`total-results`
    
    if(!length(results)){
      setMetadataEmpty()
    }
    else if(results <= 1000) {
      metadata[[i]] <- parseData(data)
    } else {
      pages <- ceiling(results/1000)
      temp_metadata <- vector("list", length(pages))
      temp_metadata[[1]] <- parseData(data)
      cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
      for(j in 2:pages){
        response <- request(startdate, enddate, issn, cursor)
        data <- fromJSON(toJSON(content(response)))
        temp_metadata[[j]] <- parseData(data)
        cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
      }
      metadata[[i]] <- bind_rows(temp_metadata)
    }
  }
  
  setTxtProgressBar(pb, i)
  
}

control_articles <- bind_rows(metadata)

rm(startdate, enddate, issn, control_doi, control_title, control_issue, control_volume, control_journal, control_publisher, df, pb, len, request) # remove redundant variables

```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(control_articles, file="data/bioRxiv_control_articles_metadata.csv", row.names=FALSE)
```

# Clean data for WoS extraction
```{r}
# Read data
published_articles_wos <- read.csv(file="data/bioRxiv_published_articles_metadata.csv", header=TRUE)
control_articles_wos <- read.csv(file="data/bioRxiv_control_articles_metadata.csv", header=TRUE)

# Convert all titles and DOIs to upper case for matching
published_articles_wos$published_doi <- toupper(published_articles_wos$published_doi)
published_articles_wos$published_title <- toupper(published_articles_wos$published_title)
control_articles_wos$control_doi <- toupper(control_articles_wos$control_doi)
control_articles_wos$control_title <- toupper(control_articles_wos$control_title)

# Remove duplicates in published articles (some authors add more than one version of a preprint)
published_articles_wos <- published_articles_wos[!duplicated(published_articles_wos$published_doi), ]

# Remove NA values in DOIs
published_articles_wos <- published_articles_wos[complete.cases(published_articles_wos$published_doi), ]
control_articles_wos <- control_articles_wos[complete.cases(control_articles_wos$control_doi), ]

# Strip line breaks
published_articles_wos$published_doi <- gsub("[\r\n]", "", published_articles_wos$published_doi)
published_articles_wos$published_title <- gsub("[\r\n]", "", published_articles_wos$published_title)
control_articles_wos$control_doi <- gsub("[\r\n]", "", control_articles_wos$control_doi)
control_articles_wos$control_title <- gsub("[\r\n]", "", control_articles_wos$control_title)

# Trim surrounding whitespace
published_articles_wos$published_doi <- trimws(published_articles_wos$published_doi)
published_articles_wos$published_title <- trimws(published_articles_wos$published_title)
control_articles_wos$control_doi <- trimws(control_articles_wos$control_doi)
control_articles_wos$control_title <- trimws(control_articles_wos$control_title)

```



```{r}
# Write results to csv
write.csv(published_articles_wos, file="data/bioRxiv_published_articles_metadata_wos.csv", row.names=FALSE)
write.csv(control_articles_wos, file="data/bioRxiv_control_articles_metadata_wos.csv", row.names=FALSE)
```
