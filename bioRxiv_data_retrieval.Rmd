---
title: "Examining the 'Early Access Effect' through bioRxiv preprints: data retrieval"
output: html_notebook
---

### This notebook provides code used to retrieve basic metadata records of bioRxiv preprints and their linked published papers from Crossref. Following extraction of this basic data, detailed metadata 


# Initial Configuration

```{r echo = T, results='hide'}
# Call libraries
library(tidyverse)
library(jsonlite)
library(lubridate)
library(curl)
library(httr)
  
# Credentials for APIs - these should be set in .Renviron file
email <- Sys.getenv("EMAIL")
altmetric_api_key <- Sys.getenv("ALTMETRIC_API_KEY")
```


```{r echo = T, results='hide'}
# Generate start and end dates of months for period of data extraction. Note that crossref API accepts dates in character format
startdates <- seq(ymd("2013-11-1"), ymd("2018-09-01"), by = "months")
enddates <- startdates %m+% months(1) - 1
months <- length(startdates)
```

# Retrieve Summary Data

### Total number of bioRxiv preprints deposited
```{r echo = T, results='hide'}
# Set start and end dates for analysis
startdate <- first(startdates)
enddate <- last(enddates)
  
# Query Crossref API for all preprints with a member id of 246 (corresponding to bioRxiv preprints) over time period
url <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,from-posted-date:", startdate, ",until-posted-date:", enddate, "&rows=0&mailto=", email, sep="")
data <- fromJSON(url)
  
#Extract counts
count_p_all <- data$message$`facets`$`publisher-name`$values$`Cold Spring Harbor Laboratory`
  
# Remove redundant variables
rm(url, data) 
```

### Total number of bioRxiv preprints deposited that have a linked published article in Crossref over time
```{r echo = T, results='hide'}
# Query Crossref API for all preprints with a member id of 246 that also have a preprint relationship to another published article (relation.type:is-preprint-of)
url <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,relation.type:is-preprint-of,from-posted-date:",startdate,",until-posted-date:",enddate,"&rows=0&mailto=", email, sep="")
data <- fromJSON(url)
  
# Extract counts
count_p_matching <- data$message$`facets`$`publisher-name`$values$`Cold Spring Harbor Laboratory`
  
# Remove redundant variables
rm(url, data, startdate, enddate) 
```

### Distribution of bioRxiv preprint deposits over time
```{r echo = T, results='hide'}
# Pre-allocate list for data
data = list()
  
# Progress bar
pb <- txtProgressBar(min = 0, max = months, initial= 0, style = 3)
  
# Iterate over months of analysis
for (i in 1:months){
  
  # Define URLs for API calls - separate calls for retrieving all preprints, and preprints that have a relationship to a published article
  url_all <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,from-posted-date:", startdates[i], ",until-posted-date:", enddates[i],"&rows=0&mailto=", email, sep="")
  url_related <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,relation.type:is-preprint-of,from-posted-date:", startdates[i], ",until-posted-date:", enddates[i], "&rows=0&mailto=", email, sep="")
  
  # Retrieve data
  response_all <- fromJSON(url_all)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  response_related <- fromJSON(url_related)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Build tibble of results
  data[[i]] <- tibble(
    "date" = format(as.Date(startdates[i]), "%Y-%m"),
    "preprints" = response_all$message$`total-results`,
    "matching_preprints" = response_related$message$`total-results`
  )
  
  setTxtProgressBar(pb, i)
  
}
  
# Bind data rows
preprint_distributions <- bind_rows(data)
  
# Remove redundant variables
rm(url_all, url_related, response_all, response_related, data, pb) 
```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(preprint_distributions, file="data/bioRxiv_preprints_monthly_distribution.csv", row.names=FALSE)
```

### Basic metadata for all bioRxiv preprints
```{r echo = T, results='hide'}
# API request function
request <- function(startdate, enddate, cursor){
  
  # URL for API call
  url <- paste("https://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,from-posted-date:", startdate, ",until-posted-date:", enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,relation,posted&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }  
}
  
# Crossref API can only serve 1000 rows per request. We loop over the dataset 1000 rows at a time and pass the cursor function to retrieve all results
loops <- ceiling(sum(preprint_distributions$preprints)/1000)
  
# Pre-allocate list to store data
metadata <- vector("list", length(loops))
  
# Set initial conditions
startdate <- first(startdates)
enddate <- last(enddates)
cursor <- "*"
  
# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)
  
for (i in 1:loops) {
  
  # Send request
  response <- request(startdate, enddate, cursor)
  
  # Retrieve data
  data <- content(response)
  data <- fromJSON(toJSON(data))
  
  # Update cursor - needs to be URL encoded
  cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
  
  # Parse basic metadata
  preprint_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  preprint_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  preprint_posted_date <- map(data$message$items$posted$`date-parts`, parseDate)
  
  # Build tibble of results
  metadata[[i]] <- tibble(
    "preprint_doi" = as.character(preprint_doi), 
    "preprint_title" = as.character(preprint_title), 
    "preprint_posted_date" = as.character(preprint_posted_date)
  )
  
  setTxtProgressBar(pb, i)
  
}
  
# Bind all data
all_preprints_metadata <- bind_rows(metadata)
  
# Remove redundant variables
rm(request, pb, i, loops, metadata, preprint_doi, preprint_title, preprint_posted_date, data, cursor, response, startdate, enddate, parseDate) 
```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(all_preprints_metadata, file="data/bioRxiv_all_preprints_metadata.csv", row.names=FALSE)
```

### Expanded metadata for bioRxiv preprints that have a link to a published paper in Crossref

```{r results='hide'}
# API request function
request <- function(startdate, enddate, cursor){
  
  # URL for API call
  url <- paste("http://api.crossref.org/types/posted-content/works?facet=publisher-name:*&filter=member:246,relation.type:is-preprint-of,from-posted-date:",startdate,",until-posted-date:",enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,group-title,relation,posted&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }
}

# Crossref API can only serve 1000 rows per request. We loop over the dataset 1000 rows at a time and pass the cursor function to retrieve all results
loops <- ceiling(sum(preprint_distributions$matching_preprints)/1000)

# Pre-allocate list to store data
metadata <- vector("list", length(loops))

# Set initial conditions
startdate <- first(startdates)
enddate <- last(enddates)
cursor <- "*"

# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

for (i in 1:loops){
  
  # Send request
  response <- request(startdate, enddate, cursor)

  # Retrieve data
  data <- content(response)
  data <- fromJSON(toJSON(data))
  
  # Update cursor - needs to be URL encoded
  cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
  
  # Parse basic metadata
  preprint_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  preprint_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  data$message$items$`group-title`[sapply(data$message$items$`group-title`, is.null)] <- NA
  preprint_category <- unlist(data$message$items$`group-title`, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  preprint_posted_date <- map(data$message$items$posted$`date-parts`, parseDate)
  
  # Parse DOI of linked published paper
  parsePublishedDOI <- function(x){
    doi <- unlist(x[["id"]], recursive = TRUE, use.names = TRUE)
  }
  published_doi <- map(data$message$items$relation$`is-preprint-of`, parsePublishedDOI)
  
  # Build tibble of results
  metadata[[i]] <- tibble(
    "preprint_doi" = as.character(preprint_doi),
    "published_doi" = as.character(published_doi),
    "preprint_title" = as.character(preprint_title), 
    "preprint_posted_date" =  as.character(preprint_posted_date),
    "preprint_category" = as.character(preprint_category)
  )
  
  setTxtProgressBar(pb, i)
}

# Bind all data
matching_preprints_metadata <- bind_rows(metadata)

# Remove redundant variables
rm(request, pb, i, loops, metadata, preprint_doi, preprint_title, preprint_posted_date, preprint_category, preprint_published_doi, data, cursor, response, startdate, enddate) 
```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(matching_preprints_metadata, file="data/bioRxiv_matching_preprints_metadata.csv", row.names=FALSE)
```

### Metadata for published articles that are linked to bioRxiv preprints via Crossref. 
```{r results='hide'}
# API request function
request <- function(doi){
  
  # URL for API call
  url <- paste("http://api.crossref.org/works/", doi, "?mailto=", email, sep="")

  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }
}

# We will loop over each individual DOI
loops = length(matching_preprints_metadata$preprint_published_doi)

# Pre-allocate list to store data
metadata <- vector("list", length(loops))

# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

for (i in 1:loops){
  
  # Send request, ensure that DOI is character encoded
  response <- request(as.character(matching_preprints_metadata$published_doi[i]))
  
  # If no response, set all variables to empty
  if(!length(response)){
    
    preprint_doi <- matching_preprints_metadata$preprint_doi[i] 
    published_doi <- matching_preprints_metadata$published_doi[i] 
    published_publisher <- NA
    published_journal <- NA
    published_title <- NA
    published_type <- NA
    published_issue <- NA
    published_volume <- NA
    published_date <- NA
    published_issn <- NA
    
  } else{
    
    # Retrieve data
    data <- content(response)
    data <- fromJSON(toJSON(data))
  
    # Extract basic metadata
    preprint_doi <- matching_preprints_metadata$preprint_doi[i]
    published_doi <- if (length(data$message$DOI)) data$message$DOI else matching_preprints_metadata$published_doi[i]
    published_publisher <- if (length(data$message$publisher)) data$message$publisher else NA
    published_type <- if (length(data$message$type)) data$message$type else NA
    published_issue <- if (length(data$message$`journal-issue`$issue)) data$message$`journal-issue`$issue else NA
    published_volume <- if (length(data$message$volume)) data$message$volume else NA
    published_title <- if (length(data$message$title)) data$message$title[1] else NA
    published_journal <- if (length(data$message$`container-title`)) data$message$`container-title`[1] else NA
    published_issn <- if (length(data$message$ISSN)) data$message$ISSN[1] else NA
      
    # Extract created date - we use the crossref 'created-date' property as an indicator of when the article was first available through the journal page
    year <- data$message$created$`date-parts`[[1]]
    month <- data$message$created$`date-parts`[[2]]
    day <- data$message$created$`date-parts`[[3]]
    published_date <- paste(year, month, day, sep="-")
  }
  
  # Build tibble of results
  metadata[[i]] <- tibble(
    "preprint_doi" = as.character(preprint_doi),
    "published_doi" = as.character(published_doi), 
    "published_publisher" = as.character(published_publisher), 
    "published_journal" = as.character(published_journal), 
    "published_title" = as.character(published_title), 
    "published_type" = as.character(published_type), 
    "published_issue" = as.character(published_issue), 
    "published_volume" = as.character(published_volume), 
    "published_date" = as.character(published_date), 
    "published_issn" = as.character(published_issn)
  )

  setTxtProgressBar(pb, i)
  
}

# Bind all data
published_articles_metadata <- bind_rows(metadata)

# Remove redundant variables
rm(pb, i, year, month, day, df, published_date, published_title, published_type, published_issue, published_volume, published_publisher, published_doi, published_journal,published_issn, data, metadata, request, loops, response, parseDate, parsePublishedDOI)

```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(published_articles_metadata, file="data/bioRxiv_published_articles_metadata.csv", row.names=FALSE)
```

### Control dataset

Generate a table of journals and ISSNs. Retrieve all articles published by those journals over our time series of analysis, including publication dates of articles

```{r}

# API request function
request <- function(startdate, enddate, issn, cursor){
  
  # URL for API call. 
  url <- paste("http://api.crossref.org/journals/", issn, "/works?filter=from-created-date:", startdate,",until-created-date:",enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,created&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response 
  if (response$status_code == 404 || !length(response)){ 
    return()
  } else  {
    return(response)
  }
}

parseData <- function(data){
  
  # Parse basic metadata
  control_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  data$message$items$title[sapply(data$message$items$title, is.null)] <- NA
  control_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  control_published_date <- map(data$message$items$created$`date-parts`, parseDate)
  
  # Build tibble of results
  metadata <- tibble(
    "control_doi" = as.character(control_doi), 
    "control_title" = as.character(control_title), 
    "control_published_date" = as.character(control_published_date)
  )
  
  return(metadata)
  
}

published_articles <- read.csv("data/bioRxiv_published_articles_metadata.csv", header=TRUE)

# Retrieve list of ISSNs to iterate over
issns <- published_articles$published_issn
issns <- as_tibble(table(issns))

loops <- length(issns$issns)

# Pre-allocate list to store data
metadata <- vector("list", length(loops))

startdate <- first(startdates)
enddate <- last(enddates)

# Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

for (i in 1:loops){
  
  cursor <- "*"
  issn <- issns$issns[i]
  
  # Send request
  response <- request(startdate, enddate, issn, cursor)
  
  # If no response, set values to empty
  if(!length(response)){
    metadata[[i]] <- tibble(
      "control_doi" = NA, 
      "control_title" = NA, 
      "control_published_date" = NA
    )
  } else {
    
    # Retrieve data
    data <- fromJSON(toJSON(content(response)))
    
    results <- data$message$`total-results`
    
    if(results <= 1000) {
      metadata[[i]] <- parseData(data)
    } else {
      pages <- ceiling(results/1000)
      temp_metadata <- vector("list", length(pages))
      temp_metadata[[1]] <- parseData(data)
      cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
      for(j in 2:pages){
        response <- request(startdate, enddate, issn, cursor)
        data <- fromJSON(toJSON(content(response)))
        temp_metadata[[j]] <- parseData(data)
        cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
      }
      metadata[[i]] <- bind_rows(temp_metadata) 
    }
  }
  setTxtProgressBar(pb, i)
}

control_articles <- bind_rows(metadata)
```





### Generate a control dataset for comparative analysis. As a first step we retrieve all articles published in the same journal-months as our dataset of articles with preprints
```{r results='hide'}

# API request function
request <- function(startdate, enddate, issn, cursor){
  
  # URL for API call. 
  url <- paste("http://api.crossref.org/journals/", issn, "/works?filter=from-created-date:",startdate,",until-created-date:",enddate,"&rows=1000&cursor=", cursor, "&select=DOI,title,publisher,container-title,volume,issue,created,type&mailto=", email, sep="")
  
  # Make request
  response <- GET(url)
  Sys.sleep(0.05) # Don't hit the API too hard! Crossref max API calls = 50/second
  
  # Check for empty response 
  if (response$status_code == 404 || !length(response)){ 
    return()
  } else  {
    return(response)
  }
}

parseData <- function(data){
  
  # Parse basic metadata
  control_doi <- unlist(data$message$items$DOI, recursive = TRUE, use.names = TRUE)
  data$message$items$title[sapply(data$message$items$title, is.null)] <- NA
  control_title <- unlist(data$message$items$title, recursive = TRUE, use.names = TRUE)
  
  # Parse preprint posted date
  parseDate <- function(x) {
    year <- unlist(x[[1]], recursive = TRUE, use.names = TRUE)
    month <- unlist(x[[2]], recursive = TRUE, use.names = TRUE)
    day <- unlist(x[[3]], recursive = TRUE, use.names = TRUE)
    date <- paste(year, month, day, sep="-")
    return(date)
  }
  control_published_date <- map(data$message$items$created$`date-parts`, parseDate)
  
  # Build tibble of results
  d <- tibble(
    "published_doi" = as.character(published_articles$published_doi[i]),
    "control_doi" = as.character(control_doi), 
    "control_title" = as.character(control_title), 
    "control_published_date" = as.character(control_published_date),
    "control_results" = data$message$`total-results`
  )
  
  return(d)
}

published_articles <- read.csv("data/bioRxiv_published_articles_metadata.csv", header=TRUE)

# Loop over each article in the published dataset
loops <- length(published_articles$published_doi)

#Progress bar
pb <- txtProgressBar(min = 0, max = loops, initial= 0, style = 3)

# Pre-allocate list to store data
# metadata <- vector("list", length(loops))

for(i in 5469:loops) {
  
  # Set initial values
  startdate <- as.Date(published_articles$published_date[i])-15
  enddate <- as.Date(published_articles$published_date[i])+15
  issn <- as.character(published_articles$published_issn[i])
  cursor <- "*"
  
  # Send request
  response <- request(startdate, enddate, issn, cursor)
  
  setMetadataEmpty <- function(){
    metadata[[i]] <- tibble(
      "published_doi" = as.character(published_articles$published_doi[i]),
      "control_doi" = NA, 
      "control_title" = NA, 
      "control_published_date" = NA,
      "control_results" = NA
    )
    return(metadata[[i]])
  }
  
  # If no response, set values to empty
  if(!length(response)){
    setMetadataEmpty()
  } else {
    
    # Retrieve data
    data <- fromJSON(toJSON(content(response)))
    
    results <- data$message$`total-results`
    
    if(!length(results)){
      setMetadataEmpty()
    }
    else if(results <= 1000) {
      metadata[[i]] <- parseData(data)
    } else {
      pages <- ceiling(results/1000)
      temp_metadata <- vector("list", length(pages))
      temp_metadata[[1]] <- parseData(data)
      cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
      for(j in 2:pages){
        response <- request(startdate, enddate, issn, cursor)
        data <- fromJSON(toJSON(content(response)))
        temp_metadata[[j]] <- parseData(data)
        cursor <- URLencode(data$message$`next-cursor`, reserved=TRUE)
      }
      metadata[[i]] <- bind_rows(temp_metadata)
    }
  }
  
  setTxtProgressBar(pb, i)
  
}

control_articles <- bind_rows(metadata)

rm(startdate, enddate, issn, control_doi, control_title, control_issue, control_volume, control_journal, control_publisher, df, pb, len, request) # remove redundant variables

```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(control_articles, file="data/bioRxiv_control_articles_metadata.csv", row.names=FALSE)
```

# Clean data for WoS extraction
```{r}
# Read data
published_articles_wos <- read.csv(file="data/bioRxiv_published_articles_metadata.csv", header=TRUE)
control_articles_wos <- read.csv(file="data/bioRxiv_control_articles_metadata.csv", header=TRUE)

# Convert all titles and DOIs to upper case for matching
published_articles_wos$published_doi <- toupper(published_articles_wos$published_doi)
published_articles_wos$published_title <- toupper(published_articles_wos$published_title)
control_articles_wos$control_doi <- toupper(control_articles_wos$control_doi)
control_articles_wos$control_title <- toupper(control_articles_wos$control_title)

# Remove duplicates in published articles (some authors add more than one version of a preprint)
published_articles_wos <- published_articles_wos[!duplicated(published_articles_wos$published_doi), ]

# Remove NA values in DOIs
published_articles_wos <- published_articles_wos[complete.cases(published_articles_wos$published_doi), ]
control_articles_wos <- control_articles_wos[complete.cases(control_articles_wos$control_doi), ]

# Strip line breaks
published_articles_wos$published_doi <- gsub("[\r\n]", "", published_articles_wos$published_doi)
published_articles_wos$published_title <- gsub("[\r\n]", "", published_articles_wos$published_title)
control_articles_wos$control_doi <- gsub("[\r\n]", "", control_articles_wos$control_doi)
control_articles_wos$control_title <- gsub("[\r\n]", "", control_articles_wos$control_title)

# Trim surrounding whitespace
published_articles_wos$published_doi <- trimws(published_articles_wos$published_doi)
published_articles_wos$published_title <- trimws(published_articles_wos$published_title)
control_articles_wos$control_doi <- trimws(control_articles_wos$control_doi)
control_articles_wos$control_title <- trimws(control_articles_wos$control_title)

```



```{r}
# Write results to csv
write.csv(published_articles_wos, file="data/bioRxiv_published_articles_metadata_wos.csv", row.names=FALSE)
write.csv(control_articles_wos, file="data/bioRxiv_control_articles_metadata_wos.csv", row.names=FALSE)
```











* Clean and merge metadata of preprints and their matching published papers
```{r echo = T, results='hide'}
# Combine matching preprints and published articles files
matching_preprints_metadata_subset <- subset(matching_preprints_metadata, select = -c(preprint_published_doi))
clean <- cbind(matching_preprints_metadata_subset, published_articles_metadata)

# Remove duplicate records:
# Convert all DOIs to lower case for matching
clean$preprint_doi <- tolower(clean$preprint_doi)
clean$published_doi <- tolower(clean$published_doi)
# How many duplicate preprints?
preprints_duplicates <- data.frame(table(clean$preprint_doi)) 
preprints_duplicates[preprints_duplicates$Freq>1,] # should be zero!
# How many duplicate published articles?
published_duplicates <- data.frame(table(clean$published_doi))
published_duplicates[published_duplicates$Freq>1,]
# Remove duplicates
clean <- clean[!duplicated(clean$preprint_doi), ]
clean <- clean[!duplicated(clean$published_doi), ]

# Remove all article types except journal articles
clean <- clean[(clean$published_type == "journal-article"),]

# Remove NA values
clean <- clean[complete.cases(clean$preprint_doi), ]

# Re-allocate row indexes
rownames(clean) <- c()

rm(matching_preprints_metadata_subset, preprints_duplicates, published_duplicates)

```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(clean, file="data/bioRxiv_preprints_published_articles_metadata.csv", row.names=FALSE)
rm(clean)

```


## Add Impact Factors to published and control article metadata

```{r echo = T, results='hide'}
# IF breakdown
merged_published <- NULL
merged_control <- NULL

impact <- read.csv("data/bioRxiv_IFs_2017.csv", header=TRUE)
impact$journal <- tolower(impact$Title)
impact <- impact[!duplicated(impact),]

# First for published articles
published_articles <- read.csv("data/bioRxiv_preprints_published_articles_metadata.csv", header=TRUE)
published_articles$published_year <- format(as.Date(published_articles$published_date, format="%Y-%m-%d"),"%Y")
published_articles$published_journal <- tolower(published_articles$published_journal)
published_articles$id <- seq.int(nrow(published_articles))

# First try to merge IF by Year and ISSN
merged_published <- merge(published_articles, impact, by.x=c("published_issn"), by.y=c("ISSN"), all.x=T)

# For records with no match, try by title
sub <- merged_published[ which(is.na(merged_published$IF)), ]
drops <- c("Rank","Title", "Cites", "IF")
sub <- sub[ , !(names(sub) %in% drops)]
sub_merged <- merge(sub, impact, by.x="published_journal", by.y="journal", all.x=T)
drops <- c("ISSN")
sub_merged <- sub_merged[ , !(names(sub_merged) %in% drops)]
merged_published <- merged_published[which(!is.na(merged_published$IF)), ]
merged_published <- rbind(merged_published, sub_merged)
merged_published <- merged_published[order(merged_published$id),]
rownames(merged_published) <- c()
drops <- c("published_year", "Rank","Title", "Cites", "journal", "id")
merged_published <- merged_published[ , !(names(merged_published) %in% drops)]

rm(sub, drops, sub_merged)

# Same for control articles
control_articles <- read.csv("data/bioRxiv_control_articles_metadata.csv", header=TRUE)
control_articles$control_year <- format(as.Date(control_articles$control_date, format="%Y-%m-%d"),"%Y")
control_articles$control_journal <- tolower(control_articles$control_journal)
control_articles$id <- seq.int(nrow(control_articles))

# First try to merge IF by Year and ISSN
merged_control <- merge(control_articles, impact, by.x=c("control_issn"), by.y=c("ISSN"), all.x=T)

# For records with no match, try by title
sub <- merged_control[ which(is.na(merged_control$IF)), ]
drops <- c("Rank","Title", "Cites", "IF")
sub <- sub[ , !(names(sub) %in% drops)]
sub_merged <- merge(sub, impact, by.x="control_journal", by.y="journal", all.x=T)
drops <- c("ISSN")
sub_merged <- sub_merged[ , !(names(sub_merged) %in% drops)]
merged_control <- merged_control[which(!is.na(merged_control$IF)), ]
merged_control <- rbind(merged_control, sub_merged)
merged_control <- merged_control[order(merged_control$id),]
rownames(merged_control) <- c()
drops <- c("control_year", "Rank","Title", "Cites", "journal", "id")
merged_control <- merged_control[ , !(names(merged_control) %in% drops)]

rm(sub, drops, sub_merged, impact, published_articles, control_articles) # remove redundant variables

```

```{r echo = T, results='hide'}
# Write results to csv
write.csv(merged_published, file="data/bioRxiv_preprints_published_articles_metadata.csv", row.names=FALSE)
write.csv(merged_control, file="data/bioRxiv_control_articles_metadata.csv", row.names=FALSE)
rm(merged_published, merged_control)
```

## OA status

* Retrieve article OA data from Unpaywall for both published and control articles
```{r echo = T, results='hide'}
# Load data
published_articles <- read.csv('data/bioRxiv_preprints_published_articles_metadata.csv', header=TRUE)
control_articles <- read.csv('data/bioRxiv_control_articles_metadata.csv', header=TRUE)

# Build request
request <- function(doi){
  url <- paste("api.unpaywall.org/v2/", doi, "?email=", email, sep="")
  response <- GET(url)
  if (response$status_code == 404) {
    return() 
  }
  else {
    data <- httr::content(response)
    return(data)
  }
}

len <- length(published_articles$published_doi)

pb <- txtProgressBar(min = 0, max = len, initial= 0, style = 3)

published_oa_status = NULL
control_oa_status = NULL

for (i in 1:len){
  
  published_doi <- as.character(published_articles$published_doi[i])
  control_doi <- as.character(control_articles$control_doi[i])
  
  # Published articles 
  data <- request(published_doi)
  if(!(length(data))){
    published_is_oa <- NA
    published_oa_journal <- NA
    published_oa_evidence <- NA
    published_oa_host_type <- NA
  }else{
    published_is_oa <- data$is_oa
    published_oa_journal <- data$journal_is_oa
    published_oa_evidence <- if (length(data$best_oa_location$evidence)) data$best_oa_location$evidence else NA
    published_oa_host_type <- if (length(data$best_oa_location$host_type)) data$best_oa_location$host_type else NA
  }
  df <- data.frame("published_doi" = published_doi, "published_is_oa" = published_is_oa, "published_oa_journal" = published_oa_journal, "published_oa_evidence" = published_oa_evidence, "published_oa_host_type" = published_oa_host_type)
  published_oa_status <- rbind(published_oa_status, df)
  
  # Control articles
  data <- request(control_doi)
  if(!(length(data))){
    control_is_oa <- NA
    control_oa_journal <- NA
    control_oa_evidence <- NA
    control_oa_host_type <- NA
  }else{
    control_is_oa <- data$is_oa
    control_oa_journal <- data$journal_is_oa
    control_oa_evidence <- if (length(data$best_oa_location$evidence)) data$best_oa_location$evidence else NA
    control_oa_host_type <- if (length(data$best_oa_location$host_type)) data$best_oa_location$host_type else NA
  }
  df <- data.frame("control_doi" = control_doi, "control_is_oa" = control_is_oa, "control_oa_journal" = control_oa_journal, "control_oa_evidence" = control_oa_evidence, "control_oa_host_type" = control_oa_host_type)
  control_oa_status <- rbind(control_oa_status, df)
  
  setTxtProgressBar(pb, i)
}

rm(len, pb, i, published_doi, control_doi, published, control, data, published_is_oa, published_oa_journal, published_oa_evidence, published_oa_host_type, control_is_oa, control_oa_journal, control_oa_evidence, control_oa_host_type, df) # remove redundant variables
```


```{r echo = T, results='hide'}
# Bind OA data to article metadata
published_oa_status <- published_oa_status[,c(2:5)]
control_oa_status <- control_oa_status[,c(2:5)]
published_articles <- cbind(published_articles, published_oa_status)
control_articles <- cbind(control_articles, control_oa_status)
```


```{r echo = T, results='hide'}
# Write results to csv
write.csv(published_articles, file="data/bioRxiv_preprints_published_articles_metadata.csv", row.names=FALSE)
write.csv(control_articles, file="data/bioRxiv_control_articles_metadata.csv", row.names=FALSE)
rm(published_oa_status, control_oa_status)

```

## Altmetrics

* Retrieve article altmetric data
```{r echo = T, results='hide'}
# Load data
published_articles <- read.csv('data/bioRxiv_preprints_published_articles_metadata.csv', header=TRUE)
control_articles <- read.csv('data/bioRxiv_control_articles_metadata.csv', header=TRUE)

# Build request for altmetrics API
request <- function(doi){
  url <- paste("https://api.altmetric.com/v1/doi/", doi, "?key=", altmetric_api_key, sep="")
  response <- GET(url)
  # Altmetric API returns 404 'Not found' for articles with no altmetric activity
  if (response$status_code == 404) {
    return() 
  }
  else {
    data <- content(response)
    return(data)
  }
}

len <- length(published_articles$published_doi)

pb <- txtProgressBar(min = 0, max = len, initial= 0, style = 3)

preprint_altmetrics = NULL
published_altmetrics = NULL
control_altmetrics = NULL

for (i in 1:len){

  preprint_doi <- as.character(published_articles$preprint_doi[i])
  published_doi <- as.character(published_articles$published_doi[i])
  control_doi <- as.character(control_articles$control_doi[i])
  
  # Preprints
  data <- request(preprint_doi)
  if(!(length(data))){
    preprint_score <- 0
    preprint_tweets <- 0
    preprint_facebook <- 0
    preprint_feeds <- 0
    preprint_mendeley <- 0
  }else{
    preprint_score <- data$score
    preprint_tweets <- if(length(data$cited_by_tweeters_count)) data$cited_by_tweeters_count else 0
    preprint_facebook <- if(length(data$cited_by_fbwalls_count)) data$cited_by_fbwalls_count else 0
    preprint_feeds <- if(length(data$cited_by_feeds_count)) data$cited_by_feeds_count else 0
    preprint_mendeley <- if(length(data$readers$mendeley)) data$readers$mendeley else 0
  }
  
  df = data.frame("preprint_score" = preprint_score, "preprint_tweets" = preprint_tweets, "preprint_facebook" = preprint_facebook, "preprint_feeds" = preprint_feeds, "preprint_mendeley" = preprint_mendeley)
  
  preprint_altmetrics = rbind(preprint_altmetrics, df)
  
  # Published articles
  data <- request(published_doi)
  if(!(length(data))){
    published_score <- 0
    published_tweets <- 0
    published_facebook <- 0
    published_feeds <- 0
    published_mendeley <- 0
  }else{
    published_score <- data$score
    published_tweets <- if(length(data$cited_by_tweeters_count)) data$cited_by_tweeters_count else 0
    published_facebook <- if(length(data$cited_by_fbwalls_count)) data$cited_by_fbwalls_count else 0
    published_feeds <- if(length(data$cited_by_feeds_count)) data$cited_by_feeds_count else 0
    published_mendeley <- if(length(data$readers$mendeley)) data$readers$mendeley else 0
  }
  
  df = data.frame("published_score" = published_score, "published_tweets" = published_tweets, "published_facebook" = published_facebook, "published_feeds" = published_feeds, "published_mendeley" = published_mendeley)
  
  published_altmetrics = rbind(published_altmetrics, df)
  
  # Control articles
  data <- request(control_doi)
  if(!(length(data))){
    control_score <- 0
    control_tweets <- 0
    control_facebook <- 0
    control_feeds <- 0
    control_mendeley <- 0
  }else{
    control_score <- data$score
    control_tweets <- if(length(data$cited_by_tweeters_count)) data$cited_by_tweeters_count else 0
    control_facebook <- if(length(data$cited_by_fbwalls_count)) data$cited_by_fbwalls_count else 0
    control_feeds <- if(length(data$cited_by_feeds_count)) data$cited_by_feeds_count else 0
    control_mendeley <- if(length(data$readers$mendeley)) data$readers$mendeley else 0
  }
    
  df = data.frame("control_score" = control_score, "control_tweets" = control_tweets, "control_facebook" = control_facebook, "control_feeds" = control_feeds, "control_mendeley" = control_mendeley)
  
  control_altmetrics = rbind(control_altmetrics, df)
    
  setTxtProgressBar(pb, i)
}

# Fix issue with control DOIs with NA values returning altmetric scores
control_altmetric$control_score[is.na(control_altmetric$control_doi)] <- NA
control_altmetric$control_tweets[is.na(control_altmetric$control_doi)] <- NA
control_altmetric$control_facebook[is.na(control_altmetric$control_doi)] <- NA
control_altmetric$control_feeds[is.na(control_altmetric$control_doi)] <- NA
control_altmetric$control_mendeley[is.na(control_altmetric$control_doi)] <- NA

rm(len, pb, i, preprint_score, preprint_tweets, preprint_facebook, preprint_feeds, preprint_mendeley, published_score, published_tweets, published_facebook, published_feeds, published_mendeley, control_score, control_tweets, control_facebook, control_feeds, control_mendeley) # remove redundant variables

```


```{r echo = T, results='hide'}
# Bind altmetrics to article metadata
published_articles <- cbind(published_articles, preprint_altmetrics)
published_articles <- cbind(published_articles, published_altmetrics)
control_articles <- cbind(control_articles, control_altmetrics)
```


```{r echo = T, results='hide'}
# Write to csv
write.csv(published_articles, file="data/bioRxiv_preprints_published_articles_metadata.csv", row.names=FALSE)
write.csv(control_articles, file="data/bioRxiv_control_articles_metadata.csv", row.names=FALSE)
rm(preprint_altmetrics, published_altmetrics, control_altmetrics) # remove redundant variables
```






