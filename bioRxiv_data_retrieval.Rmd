---
title: "Examining the citation and altmetric advantage of bioRxiv preprints: bioRxiv data retrieval"
output: html_notebook
---

# Initial Configuration --------------------------------------------------------

### Load libraries

```{r results='hide'}

library(tidyverse)
library(lubridate)
library(stringdist)
library(rcrossref)
library(RSelenium)
library(rvest)
library(roadoi)
library(RODBC)
library(fuzzyjoin)
library(countrycode)

```

### Credentials

```{r results='hide'}

# These should be set in .Renviron file
email <- Sys.getenv("EMAIL")
altmetric_api_key <- Sys.getenv("ALTMETRIC_API_KEY")
gender_api_key <- Sys.getenv("GENDER_API_KEY")
kb_username <- Sys.getenv("KB_USERNAME")
kb_password <- Sys.getenv("KB_PASSWORD")

```

### Global variables and helper functions

```{r results='hide'}

# For cleaning strings for improved matching efficiency (e.g. titles)
cleanString <- function(string) {
  string <- gsub("<.*?>", "", string) # remove tags
  string <- gsub("[\r\n]", "", string) # remove line breaks
  string <- str_replace_all(string,"[^[:graph:]]", " ") # remove non UTF-8 encoded characters
  string <- str_trim(string) # remove whitespace
  string <- tolower(string) # set lower case
  return(as.character(string)) # return as character vector
}

# For retrieving dates from date-parts returned by Crossref API
parseDate <- function(parts) {
  as.character(as_date(ymd(paste0(parts[1], "-", parts[2], "-", parts[3]))))
}

# For parsing and subsampling abstracts (e.g. first 100 characters)
parseAbstract <- function(abstract, length = 100) {
  substr <- substr(cleanString(abstract), 1, length)
}

```

# Preprint Data ----------------------------------------------------------------

```{r}

# Generate start and end dates of months for period of data extraction
startdates <- seq(ymd("2013-11-1"), ymd("2017-12-01"), by = "months")
enddates <- startdates %m+% months(1)
months <- length(startdates)
startdate <- startdates[1]
enddate <- enddates[length(enddates)]

```

### bioRxiv preprint depositions according to Crossref

```{r results='hide'}

# Total number of bioRxiv preprints in Crossref
count_all_preprints <- cr_types(types="posted-content", works=TRUE, 
                                facet="publisher-name:*", 
                                filter=c(member = 246, 
                                from_posted_date = as.character(startdate), 
                                until_posted_date = as.character(enddate)),
                                limit=0)$facets$`publisher-name`$V1

# Number of preprints with a link to a published paper in Crossref
count_published_preprints <- cr_types(types="posted-content", works=TRUE, 
                                      facet="publisher-name:*", 
                                      filter=c(member = 246, 
                                      relation.type = "is-preprint-of", 
                                      from_posted_date = as.character(startdate), 
                                      until_posted_date = as.character(enddate)),
                                      limit=0)$facets$`publisher-name`$V1

```

# Article Metadata Extraction --------------------------------------------------

### Crossref metadata for all bioRxiv preprints

```{r results='hide'}

# Retrieve DOI, title, posted (issued) date and authorship for preprints
data <- cr_types(types="posted-content", works=TRUE, 
                 facet="publisher-name:*", filter=c(member = 246, 
                 from_posted_date = as.character(startdate), 
                 until_posted_date = as.character(enddate)), 
                 limit=1000, cursor="*", cursor_max = count_all_preprints, 
                 select=c("DOI", "title", "issued", "author", "abstract"))$data

# Build data frame
preprints <- tibble(
  "PREPRINT_DOI" = cleanString(data$doi),
  "PREPRINT_TITLE" = cleanString(data$title),
  "PREPRINT_POSTED_DATE" = as_date(data$issued),
  "PREPRINT_POSTED_YEAR" = year(as_date(data$issued)),
  # We only want to keep the family name and first initial of the first author
  "PREPRINT_FIRST_AUTHOR" = map_chr(data$author, 
                                    function(x) cleanString(paste0(x$family[1],                                                                      str_sub(x$given[1],1,1)))),
  "PREPRINT_AUTHOR_COUNT" = map_int(data$author, 
                                    function(x) length(x$family)),
  "PREPRINT_ABSTRACT_SHORT" = map_chr(data$abstract, parseAbstract)
)

# Write results to csv
write.csv(preprints %>% select(PREPRINT_DOI, PREPRINT_TITLE, PREPRINT_POSTED_DATE), 
          file="data/preprints.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(data)

```

### We match preprints to published papers via 3 sets of links:

### 1. Using the Crossref 'relationship' property
### 2. Direct web scraping of bioRxiv website
### 3. Fuzzy matching of titles and abstracts of bioRxiv preprints and Scopus

### 1. DOI links between preprints and published papers via Crossref

```{r results='hide'}

# To retrieve relationships, we use the rcrossref low level API (cr_types_)
data <- cr_types_(types="posted-content", works=TRUE, 
                  filter=c(member = 246, relation.type = "is-preprint-of", 
                  from_posted_date = as.character(startdate), 
                  until_posted_date = as.character(enddate)), 
                  cursor="*", limit=1000, cursor_max=count_published_preprints, 
                  parse=TRUE, select=c("DOI", "relation", "issued", "publisher"))

# Extract individual items
items <- unlist(lapply(data, function(x) x$message$items), recursive=FALSE)

# Build data frame
preprints_published_articles_cr <- tibble(
  "PREPRINT_DOI" = map_chr(items, function(x) cleanString(x$DOI)),
  "ARTICLE_DOI" = map_chr(items, function(x) 
                          cleanString(x$relation$`is-preprint-of`[[1]]$id)))

# Write results to csv
write.csv(preprints_published_articles_cr, 
          file="data/preprints_published_articles_cr.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(data, items)

```

### 2. DOI links between preprints and published papers via bioRxiv website

```{r}

# Initialise RSelenium client
rD <- rsDriver(verbose = FALSE, port=4444L, browser="firefox")
remDr <- rD$client
remDr$open(silent = TRUE)

# Function for retrieving data from bioRxiv webpages
retrieveData <- function(preprint_doi) {
  
  # Query url - we use the doi which redirects to bioRxiv
  url <- paste0("https://doi.org/", preprint_doi)
  
  result <- tryCatch({
      
    # Navigate to url
    remDr$navigate(url)
    
    #Read page html
    html <- read_html(remDr$getPageSource()[[1]])
      
    # Parse relevant html node
    doi <- html %>%
      html_node("div.pub_jnl > a") %>%
      html_text()
      
    # Return data as tibble
    published_articles_bio <- tibble(
      "PREPRINT_DOI" = preprint_doi,
      "ARTICLE_DOI_BIO" = cleanString(doi)
    )
    published_articles_bio
  },
  error = function(e) {
    # If call results in error, set data to NA
    published_articles_bio <- tibble(
      "PREPRINT_DOI" = preprint_doi,
      "ARTICLE_DOI" = NA
    )
    published_articles_bio
  })
  # Return result
  result
}

# Map results to dataframe, keep only valid DOIs
preprints_published_articles_bio <- map_dfr(preprints$preprint_doi,
                                            retrieveData) %>% 
  filter(substr(article_doi_bio, 1, 3) == "10.")

# Write results to csv
write.csv(preprints_published_articles_bio, 
          file="data/preprints_published_articles_bio.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Close Selenium client
remDr$close()

rm(retrieveData, rD, remDr)

```

### 3. Matching of preprints to Scopus records via fuzzy title and abstract matching

```{r results='hide'}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Save the preprints to an SQL table
sqlSave(con, preprints, "BIORXIV_PREPRINTS",
        rownames=FALSE,
        varTypes=c(PREPRINT_DOI= "varchar(32)",
                   PREPRINT_TITLE= "varchar(1024)",
                   PREPRINT_POSTED_DATE = "date",
                   PREPRINT_POSTED_YEAR = "number(4,0)",
                   PREPRINT_FIRST_AUTHOR = "varchar(128)",
                   PREPRINT_AUTHOR_COUNT = "number(4,0)",
                   PREPRINT_ABSTRACT_SHORT = "varchar(128)"))

# Create a table of potentially matching items
sqlQuery(con, read_file("sql/preprints_scopus_fuzzy_matching.sql"))

# Read titles and abstracts of scopus items for matching
scp_items <- as_tibble(sqlQuery(con, read_file("sql/preprints_scopus_titles_abstracts.sql"))) %>%
                 distinct(PK_ITEMS, .keep_all=T) %>%
                 mutate(DOI = cleanString(DOI),
                        AUTHORNAME = cleanString(AUTHORNAME),
                        ARTICLE_TITLE = cleanString(ARTICLE_TITLE),
                        ABSTRACT_SHORT = cleanString(ABSTRACT_SHORT))

# Close DB connection
odbcClose(con)

# Fuzzy matching function. Returns best article match and the matching accuracy
getMatch <- function(p_author, p_year, p_count, p_title, p_abstract) {
  
  # Filter Scopus articles for author, year and author count
  items <- scp_items %>%
    filter(AUTHORNAME == p_author,
           PUBYEAR >= p_year,
           AUTHOR_CNT == p_count)
  
  if(!length(items$AUTHORNAME)){
    return(NA)
  } else {
    # Fuzzy matching, method "jw" = Jaro-Winkler distance
    # maxDist of 0.2 represents 80 % similarity
    # Try match on title
    index <- amatch(p_title, 
                    items$ARTICLE_TITLE, 
                    method="jw", 
                    nomatch = NA_integer_, 
                    maxDist = 0.2)
    # If no match on title, try on abstract
    if(is.na(index)) {
      index <- amatch(p_abstract, 
                    items$ABSTRACT_SHORT, 
                    method="jw", 
                    nomatch = NA_integer_, 
                    maxDist = 0.2)
    } 
    # If no match on title or abstract, return NA
    if(is.na(index)) {
      return(NA)
    }
    # Otherwise generate match details
    else {
      match <- as_tibble(items[index,]) %>%
        pull(DOI)
      return(match)
    }
  }
}

# Match preprint titles to articles in Scopus
preprints_published_articles_scp <- preprints %>%
  rowwise() %>%
  mutate(ARTICLE_DOI = pmap_chr(list(PREPRINT_FIRST_AUTHOR,
                         PREPRINT_POSTED_YEAR,
                         PREPRINT_AUTHOR_COUNT,
                         PREPRINT_TITLE,
                         PREPRINT_ABSTRACT_SHORT),
                    getMatch)) %>%
  ungroup() %>%
  select(PREPRINT_DOI, ARTICLE_DOI) %>%
  filter(ARTICLE_DOI != "NA")


# Write to csv
write.csv(preprints_published_articles_scp, 
          "data/preprints_published_articles_scp.csv",
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(con, scp_items, getMatch)

```

### Merge preprint-published article links

```{r results='hide'}

# Prepare data for merging - add source field
preprints_published_articles_bio <- preprints_published_articles_bio %>%
  mutate(SOURCE = "bioRxiv")
preprints_published_articles_cr <- preprints_published_articles_cr %>%
  mutate(SOURCE = "Crossref")
preprints_published_articles_scp <- preprints_published_articles_scp %>%
  mutate(SOURCE = "Scopus")

# Merge all preprint-published article links.
# For rows with the same published doi but different preprint doi we select the
# earliest posted preprint. This likely occurs when authors upload multiple
# preprints as separate versions.
# For rows with the same preprint doi but different published dois we designate
# a priority list (biorxiv > crossref > scopus) and select the top result.
preprints_published_articles <- bind_rows(preprints_published_articles_bio,
                                          preprints_published_articles_cr,
                                          preprints_published_articles_scp) %>%
  distinct(PREPRINT_DOI, ARTICLE_DOI, .keep_all=T) %>%
  inner_join(select(preprints, PREPRINT_DOI, PREPRINT_POSTED_DATE)) %>%
  group_by(ARTICLE_DOI) %>%
  arrange(PREPRINT_POSTED_DATE, SOURCE) %>%
  slice(1) %>%
  ungroup() %>%
  group_by(PREPRINT_DOI) %>%
  arrange(SOURCE) %>%
  slice(1) %>%
  ungroup() %>%
  select(-SOURCE, -PREPRINT_POSTED_DATE)

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Save preprints_published_articles to a new SQL table
sqlSave(con, preprints_published_articles, "BIORXIV_PRE_PUB", rownames=FALSE,
        varTypes=c(PREPRINT_DOI="varchar(512)", ARTICLE_DOI="varchar(512)"))

# Close DB connection
odbcClose(con)

# Write to csv
write.csv(preprints_published_articles, 
          "data/preprints_published_articles.csv",
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(preprints_published_articles_bio, preprints_published_articles_cr,
   preprints_published_articles_scp, con, query)

```

### For citation and altmetrics analysis: match published articles to Scopus records

```{r results='hide'}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Create table of matched articles
sqlQuery(con, read_file("sql/preprints_published_articles.sql"))

# For a small number of articles there are two entries (items) in Scopus per DOI.
# We want to remove these - usually one item is the 'false' item and is never linked 
# to in citations. Thus, we find the duplicate items and remove the one that has
# the lowest (or usually zero) citations.
sqlQuery(con, read_file("sql/remove_duplicate_published_articles.sql"))

# Close DB connection
odbcClose(con)

# Remove redundant variables
rm(con)

```

# Generating a Control Dataset -------------------------------------------------

### Retrieve all articles published in same journal as our published articles
### Most of this is done in the KB database directly

```{r}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Create a table of all potential control articles from same journals
sqlQuery(con, read_file("sql/all_control_articles.sql"))

# Clean controls
sqlQuery(con, read_file("sql/clean_controls.sql"))

# Create table of single category articles
sqlQuery(con, read_file("sql/single_category_articles.sql"))

# Retrieve controls for single category articles
single_category_controls <- as_tibble(sqlQuery(con, 
                              read_file("sql/single_category_controls.sql")))

# Create table of multi category articles
sqlQuery(con, read_file("sql/multi_category_articles.sql"))

# Create table of categories for published multi-category articles
sqlQuery(con, read_file("sql/published_multi_category_cited.sql"))

# Create table of categories for control multi-category articles
sqlQuery(con, read_file("sql/control_multi_category_cited.sql"))

# Retrieve controls for multi category articles
multi_category_controls <- as_tibble(sqlQuery(con, 
                            read_file("sql/multi_category_controls.sql")))

# Close DB connection
odbcClose(con)
  
# Bind all data together
control_articles <- bind_rows(single_category_controls,
                              multi_category_controls) %>%
  mutate(ARTICLE_DOI = cleanString(ARTICLE_DOI))

# Remove redundant variables
rm(query, con, single_category_controls, multi_category_controls)

```

### Create final analysis datasets

```{r results='hide'}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Retrieve all bioRxiv-deposited articles
published_articles <- sqlFetch(con, "BIORXIV_PUBLISHED_ARTICLES") %>%
  # Add a TYPE field to distinguish deposited from control articles
  mutate(TYPE = "Deposited") %>%
  select(-FK_ITEMS, -UT_EID, -FK_SOURCES) %>%
  distinct(PREPRINT_DOI, .keep_all = T)

# Close DB connection
odbcClose(con)

# Retrieve all control articles
control_articles_analysis <- control_articles %>%
  mutate(TYPE = "Control") %>%
  select(-FK_ITEMS, -UT_EID, -FK_SOURCES) %>%
  distinct(PREPRINT_DOI, .keep_all = T) %>%
  # Make sure that control articles and deposited articles correspond
  filter(PREPRINT_DOI %in% published_articles$PREPRINT_DOI)

published_articles_analysis <- published_articles %>%
  filter(PREPRINT_DOI %in% control_articles_analysis$PREPRINT_DOI)

# Bind articles and clean up journal names
articles <- bind_rows(published_articles_analysis, control_articles_analysis) %>%
  mutate(SOURCETITLE = cleanString(SOURCETITLE))

# Save analysis set to new table
sqlSave(con, articles, "BIORXIV_ALL_ARTICLES", rownames=FALSE, 
        varTypes=c(PREPRINT_DOI="varchar(512)", ARTICLE_DOI="varchar(512)",
        ARTICLE_CREATED_DATE = "date", ARTICLE_TITLE = "varchar(2048)"))

# Write to csv
write.csv(articles, 
          file="data/articles.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(con, published_articles, control_articles, published_articles_analysis, control_articles_analysis)
```

### Citing article data

```{r}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Retrieve all citing articles and their publication dates
citing_articles <- as_tibble(sqlQuery(con,read_file("sql/citing_articles.sql"))) %>%
  mutate(CITED_ARTICLE_DOI = cleanString(CITED_ARTICLE_DOI),
         CITING_ARTICLE_DOI = cleanString(CITING_ARTICLE_DOI))

# Close DB connection
odbcClose(con)

# Remove redundant variables
rm(con)

write.csv(citing_articles, 
          file="data/citing_articles.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

```

### Altmetric data

```{r}

# API request function
request <- function(doi){
  
  # URL for API call
  url <- paste("http://api.altmetric.com/v1/doi/", 
               doi, sep="")

  # Make request
  response <- httr::GET(url)
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  }else{
    return(response)
  }
}

# Function for retrieving altmetrics
getAltmetrics <- function(doi) {
  
  response <- request(doi)
  
  # If no response, set all variables to NA
  if(!length(response)){
    
    altmetrics_data <- tibble(
      "ARTICLE_DOI" = doi,
      "HAS_ALTMETRIC" = F,
      "ALT_SCORE" = NA,
      "ALT_TWEETS" = NA, 
      "ALT_FEEDS" = NA,
      "ALT_MSM" = NA,
      "ALT_POLICIES" = NA,
      "ALT_WIKIPEDIA" = NA,
      "ALT_MENDELEY" = NA
    )
    
  } else {

    # Retrieve data
    data <- httr::content(response, as="parsed")
    
    # Build tibble of results
    # Where no data exists on a field, we set it to NA
    altmetrics_data <- tibble(
      "ARTICLE_DOI" = doi,
      "HAS_ALTMETRIC" = T,
      "ALT_SCORE" = if (length(data$score)) data$score else NA,
      "ALT_TWEETS" = if (length(data$cited_by_tweeters_count)) data$cited_by_tweeters_count else NA, 
      "ALT_FEEDS" = if (length(data$cited_by_feeds_count)) data$cited_by_feeds_count else NA,
      "ALT_MSM" = if (length(data$cited_by_msm_count)) data$cited_by_msm_count else NA,
      "ALT_POLICIES" = if (length(data$cited_by_policies_count)) data$cited_by_policies_count else NA,
      "ALT_WIKIPEDIA" = if (length(data$cited_by_wikipedia_count)) data$cited_by_wikipedia_count else NA,
      "ALT_MENDELEY" = if (length(data$readers$mendeley)) as.numeric(data$readers$mendeley) else NA
    )
  
  }
  
  return(altmetrics_data)
}

# Retrieve altmetric data for all published articles
article_altmetrics <- map_dfr(articles$ARTICLE_DOI,
                              function(x) getAltmetrics(x)) %>%
  distinct(ARTICLE_DOI, .keep_all=T) %>% 
  left_join(select(articles, PREPRINT_DOI, ARTICLE_DOI), 
            by=c("ARTICLE_DOI")) %>%
  replace(., is.na(.), 0) %>%
  select(PREPRINT_DOI, ARTICLE_DOI, HAS_ALTMETRIC, ALT_SCORE, ALT_TWEETS, 
         ALT_FEEDS, ALT_MSM, ALT_POLICIES, ALT_WIKIPEDIA, ALT_MENDELEY)

# Retrieve altmetric data for preprints
preprint_altmetrics <- map_dfr(preprints$PREPRINT_DOI, 
                               function(x) getAltmetrics(x)) %>%
  rename(PREPRINT_DOI = ARTICLE_DOI) %>%
  replace(., is.na(.), 0)

# Write to csv
write.csv(article_altmetrics,
          file="data/article_altmetrics.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")
write.csv(preprint_altmetrics,
          file="data/preprint_altmetrics.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

```

### Article OA status

```{r results='hide'}

# Retrive the OA status of articles from Unpaywall (OAdoi)
# We keep only the IS_OA and JOURNAL_IS_OA properties
article_oa_status <- oadoi_fetch(dois = articles$ARTICLE_DOI, 
                    email = email) %>%
  left_join(select(articles, PREPRINT_DOI, ARTICLE_DOI), by=c("doi" = "ARTICLE_DOI")) %>%
  select(PREPRINT_DOI, doi, is_oa, journal_is_oa) %>%
  rename(ARTICLE_DOI = doi,
         IS_OA = is_oa,
         JOURNAL_IS_OA = journal_is_oa)

# write to csv
write.csv(article_oa_status,
          file="data/article_oa_status.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

```

### Preprint citation data

```{r}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# First we retrieve all potential citations and conduct some initial cleaning steps
data <- sqlQuery(con, read_file("sql/preprint_citing_articles.sql")) %>%
  mutate(REFTITLE = cleanString(REFTITLE),
         SOURCETITLE = cleanString(SOURCETITLE))

# Close DB connection
odbcClose(con)

# We combine different methods to match references to preprints:

# Match reference titles to preprint titles
reftitle_matches_title <- data %>%
  mutate_all(~gsub("biorxiv", "", .)) %>%
  stringdist_inner_join(preprints, by=c("REFTITLE" = "PREPRINT_TITLE"), method="jw", max_dist = 0.2) %>%
  select(PREPRINT_DOI, PREPRINT_POSTED_DATE, CITING_ARTICLE_DOI, CITING_ARTICLE_CREATED_DATE)

# Sometimes titles are given by mistake in the source title - match these to preprint titles
sourcetitle_matches_title <- data %>%
  mutate_all(~gsub("biorxiv", "", .)) %>%
  stringdist_inner_join(preprints, by=c("SOURCETITLE" = "PREPRINT_TITLE"), method="jw", max_dist = 0.2) %>%
  select(PREPRINT_DOI, PREPRINT_POSTED_DATE, CITING_ARTICLE_DOI, CITING_ARTICLE_CREATED_DATE)

# Match where an exact DOI is given (e.g. "https://doi.org/12345/6789")
sourcetitle_matches_doi_exact <- data %>%
  mutate_all(~gsub("biorxiv", "", .)) %>%
  mutate(DOI = str_extract(SOURCETITLE, "([0-9]+).*$")) %>%
  inner_join(preprints, by=c("DOI" = "PREPRINT_DOI")) %>%
  rename(PREPRINT_DOI = DOI) %>%
  select(PREPRINT_DOI, PREPRINT_POSTED_DATE, CITING_ARTICLE_DOI, CITING_ARTICLE_CREATED_DATE)

# Match where an partial DOI is given (e.g. "12345/6789")
sourcetitle_matches_doi_partial <- data %>%
  mutate_all(~gsub("biorxiv", "", .)) %>%
  mutate(NUMBER = str_extract(SOURCETITLE, "([0-9]+).*$")) %>%
  mutate(DOI = paste0("10.1101/", NUMBER)) %>%
  inner_join(preprints, by=c("DOI" = "PREPRINT_DOI")) %>%
  rename(PREPRINT_DOI = DOI) %>%
  select(PREPRINT_DOI, PREPRINT_POSTED_DATE, CITING_ARTICLE_DOI, CITING_ARTICLE_CREATED_DATE)

# Bind all matches together
preprint_citing_articles <- bind_rows(reftitle_matches_title,
                         sourcetitle_matches_title,
                         sourcetitle_matches_doi_exact,
                         sourcetitle_matches_doi_partial) %>% distinct()

# Write to csv
write.csv(preprint_citing_articles,
          file="data/preprint_citing_articles.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(reftitle_matches_title, sourcetitle_matches_title, sourcetitle_matches_doi_exact, sourcetitle_matches_doi_partial)
```

### Impact Factors

Impact factors are calculated directly from Scopus data

```{r}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

impact_factors <- as_tibble(sqlQuery(con, read_file("sql/impact_factors.sql")))

# Close DB connection
odbcClose(con)

# Write to csv
write.csv(impact_factors,
          file="data/impact_factors.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

# Remove redundant variables
rm(con)

```

### Authorship data

### Author countries

```{r}
# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Retrieve countries associated with first authors 
# Note: an author can have more than one country
first_author_countries <- as_tibble(sqlQuery(con, read_file("sql/first_author_countries.sql")))
last_author_countries <- as_tibble(sqlQuery(con, read_file("sql/last_author_countries.sql")))

# Join author countries back to articles
first_author_countries <- articles %>% 
  left_join(first_author_countries, by="ARTICLE_DOI") %>% 
  distinct(PREPRINT_DOI, ARTICLE_DOI, COUNTRYCODE)
last_author_countries <- articles %>% 
  left_join(last_author_countries, by="ARTICLE_DOI") %>% 
  distinct(PREPRINT_DOI, ARTICLE_DOI, COUNTRYCODE)

# Close DB connection
odbcClose(con)

# Remove redundant variables
rm(query, con)

# write to csv
write.csv(first_author_countries,
          file="data/first_author_countries.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")
write.csv(last_author_countries,
          file="data/last_author_countries.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")
```

### Author Academic Age and Productivity

```{r}

# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# First create a temporary table of all Author IDs for first and last authors
sqlQuery(con, read_file("sql/first_authors_ids.sql"))
sqlQuery(con, read_file("sql/last_authors_ids.sql"))

# Retrieve academic ages of authors from Scopus
first_author_age <- as_tibble(sqlQuery(con, read_file("sql/first_author_age.sql")))
last_author_age <- as_tibble(sqlQuery(con, read_file("sql/last_author_age.sql")))

# Join authors back to articles. Some authors come back with more than one value,
# if Scopus has two records for the same author. In this case we take the highest
# age
first_author_age <- articles %>% 
  left_join(first_author_age, by="ARTICLE_DOI") %>% 
  distinct(PREPRINT_DOI, ARTICLE_DOI, AUTHOR_AGE) %>% 
  group_by(PREPRINT_DOI, ARTICLE_DOI) %>% 
  arrange(desc(AUTHOR_AGE)) %>% 
  top_n(1) %>% 
  ungroup()
last_author_age <- articles %>% 
  left_join(last_author_age, by="ARTICLE_DOI") %>% 
  distinct(PREPRINT_DOI, ARTICLE_DOI, AUTHOR_AGE) %>% 
  group_by(PREPRINT_DOI, ARTICLE_DOI) %>% 
  arrange(desc(AUTHOR_AGE)) %>% 
  top_n(1) %>% 
  ungroup()
# Close DB connection
odbcClose(con)

# Remove redundant variables
rm(con)

# write to csv
write.csv(first_author_age,
          file="data/first_author_age.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")
write.csv(last_author_age,
          file="data/last_author_age.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

```

### Author Gender

```{r}
# Open DB connection
con <- odbcConnect("KB", uid=kb_username, pwd=kb_password)

# Retireve first author names from Scopus
first_author_names <- as_tibble(sqlQuery(con, read_file("sql/first_author_names.sql")))
last_author_names <- as_tibble(sqlQuery(con, read_file("sql/last_author_names.sql")))

# Close DB connection
odbcClose(con)

# Merge names and countries to produce a list of distinct names/countries
first_author_names_countries <- first_author_names %>%
  inner_join(first_author_countries, by="ARTICLE_DOI") %>%
  select(FIRSTNAME, COUNTRYCODE) %>%
  distinct(FIRSTNAME, COUNTRYCODE)
last_author_names_countries <- last_author_names %>%
  inner_join(last_author_countries, by="ARTICLE_DOI") %>%
  select(FIRSTNAME, COUNTRYCODE) %>%
  distinct(FIRSTNAME, COUNTRYCODE)

# Function for retrieving gender from Gender API
getGenderFromName<- function(FIRSTNAME, COUNTRYCODE) {
  
  # If a country code is present, use a localized query
  if(!is.na(COUNTRYCODE)){
    # Convert countrycode to ISO3
    COUNTRY = countrycode(COUNTRYCODE, "iso3c", "iso2c")
    url <- paste0("https://gender-api.com/get?name=", FIRSTNAME, "&country=", 
                COUNTRY, "&key=", gender_api_key)
  }
  # Otherwise use a general query
  else {
     url <- paste0("https://gender-api.com/get?name=", FIRSTNAME, 
                   "&key=", gender_api_key)
  }
  
  # Make request
  response <- httr::GET(url)
  gender <- httr::content(response)$gender 
  
  # If gender unknown, try without localization parameter
  if(isTRUE(gender == "unknown")) {
    url <- paste0("https://gender-api.com/get?name=", FIRSTNAME, 
                   "&key=", gender_api_key)
    response <- httr::GET(url)
    gender <- httr::content(response)$gender 
  }
  
  # Check for empty response (404)
  if (response$status_code == 404) { 
    return()
  # Otherwise return gender
  }else{
    return(
      tibble(
        "NAME" = FIRSTNAME,
        "COUNTRYCODE" = COUNTRYCODE,
        "GENDER" = if(length(gender)) gender else NA
      )
    )
  }
}

# First we make a list of all potential name/country combinations, to reduce the
# total number of API calls we need to make
first_author_names_gender <- first_author_names_countries %>% 
  # Regex to strip initials
  mutate(FIRSTNAME = gsub("(?:[A-Z]\\.){1,3},?", "", FIRSTNAME)) %>%
  # Remove blanks
  filter(nchar(FIRSTNAME) > 0) %>%
  # Keep only distinct name-country values
  distinct(FIRSTNAME, COUNTRYCODE) %>%
  # Remove factors from COUNTRYCODE
  mutate(COUNTRYCODE = as.character(COUNTRYCODE)) %>%
  pmap_dfr(., getGenderFromName)
last_author_names_gender <- last_author_names_countries %>% 
  # Regex to strip initials
  mutate(FIRSTNAME = gsub("(?:[A-Z]\\.){1,3},?", "", FIRSTNAME)) %>%
  # Remove blanks
  filter(nchar(FIRSTNAME) > 0) %>%
  # Keep only distinct name-country values
  distinct(FIRSTNAME, COUNTRYCODE) %>%
  # Remove factors from COUNTRYCODE
  mutate(COUNTRYCODE = as.character(COUNTRYCODE)) %>%
  pmap_dfr(., getGenderFromName)

# Genders are joined back to articles from the name/country/gender relationships
# retrieved above. Where no gender is returned, we set gender to 'unknown'. A
# very small number of queries return more than one result, in cases of 
# conflicting information, we also set the gender to 'unknown'.
first_author_gender <- articles %>%
  left_join(first_author_names, by="ARTICLE_DOI") %>%
  distinct() %>%
  mutate(FIRSTNAME = gsub("(?:[A-Z]\\.){1,3},?", "", FIRSTNAME)) %>%
  left_join(first_author_countries, by=c("PREPRINT_DOI", "ARTICLE_DOI")) %>%
  left_join(first_author_names_gender, by=c("FIRSTNAME" = "NAME",
                                             "COUNTRYCODE")) %>%
  distinct(PREPRINT_DOI, ARTICLE_DOI, GENDER) %>%
  replace(., is.na(.), "unknown") %>%
  group_by(PREPRINT_DOI, ARTICLE_DOI) %>%
  mutate(COUNT = n()) %>%
  mutate(GENDER = ifelse(COUNT < 2, GENDER, "unknown")) %>%
  select(-COUNT) %>%
  ungroup() %>%
  distinct(PREPRINT_DOI, ARTICLE_DOI, GENDER)
last_author_gender <- articles %>%
  left_join(last_author_names, by="ARTICLE_DOI") %>%
  distinct() %>%
  mutate(FIRSTNAME = gsub("(?:[A-Z]\\.){1,3},?", "", FIRSTNAME)) %>%
  left_join(last_author_countries, by=c("PREPRINT_DOI", "ARTICLE_DOI")) %>%
  left_join(last_author_names_gender, by=c("FIRSTNAME" = "NAME",
                                             "COUNTRYCODE")) %>%
  distinct(PREPRINT_DOI, ARTICLE_DOI, GENDER) %>%
  replace(., is.na(.), "unknown") %>%
  group_by(PREPRINT_DOI, ARTICLE_DOI) %>%
  mutate(COUNT = n()) %>%
  mutate(GENDER = ifelse(COUNT < 2, GENDER, "unknown")) %>%
  select(-COUNT) %>%
  ungroup() %>%
  distinct(PREPRINT_DOI, ARTICLE_DOI, GENDER)

# write to csv
write.csv(first_author_gender,
          file="data/first_author_gender.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")
write.csv(last_author_gender,
          file="data/last_author_gender.csv", 
          row.names=FALSE, 
          fileEncoding="UTF-8")

```

